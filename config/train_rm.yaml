name: RM
distributed: false
train_dataset:
  dataset_name: "ST"
  corrupted_dir: "D:/dataset/synth80k/test/t_incomplete"
  gt_dir: "D:/dataset/synth80k/test/mask_t"
  resolution: [64, 288]
  batch_size: 8
  num_workers: 4
  use_shuffle: true
val_dataset:
  input_dir: null
  output_dir: null
  resolution: [64, 288]
gpu_ids:
- 0
model:
  beta_schedule:  # use munual beta_schedule for acceleration
    train:
      linear_end: 0.01
      linear_start: 1.0e-06
      n_timestep: 2000
      schedule: linear
    val:
      linear_end: 0.01
      linear_start: 1.0e-06
      n_timestep: 2000
      schedule: linear
  diffusion:
    channels: 3
    conditional: true  # unconditional generation or unconditional generation
    image_size:
    - 288
    - 288
    sampling_timesteps: 1
  finetune_norm: false
  unet:
    attn_res:
    - 16
    channel_multiplier:
    - 1
    - 2
    - 4
    - 8
    - 8
    dropout: 0.2
    in_channel: 9
    inner_channel: 64
    out_channel: 3
    res_blocks: 2
  which_model_G: sr3  # use the ddpm or sr3 network structure
path:
  log: 'logs'
  results: results
  resume_state: null  # If the ckpt file is "XXX/rm_gen.pth",  you should write "XXX/rm" here.
  tb_logger: tb_logger
  SPM_pretrain: "checkpoints/spm.pt"
phase: train
SPM_cum: 32
train:
  ema_scheduler:  # not used now
    ema_decay: 0.9999
    step_start_ema: 5000
    update_ema_every: 1
  n_iter: 500000
  optimizer:
    lr: 0.0001
    type: adam
  print_freq: 200
  save_checkpoint_freq: 2500.0
  val_freq: 2500
  n_validation_images: 8

